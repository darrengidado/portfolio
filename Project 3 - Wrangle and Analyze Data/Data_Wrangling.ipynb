{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenStreetMap Data Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Area\n",
    "\n",
    "San Jose, CA, United States\n",
    "\n",
    "- https://mapzen.com/data/metro-extracts/ (Metro Extract has been discontinued as of Feb 1st, 2018)\n",
    "- https://gist.github.com/carlward/54ec1c91b62a5f911c42#file-sample_project-md (Sample SQL project)\n",
    "- https://docs.python.org/3/library/sqlite3.html (Python documentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems Encountered in the Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "from collections import defaultdict\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I first run the code to find out how many different types of tags are there and the count of each tag.\n",
    "\n",
    "It helps to provide an overview of the amount of data in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'member': 26, 'osm': 1, 'relation': 1, 'tag': 29}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "The code below is to find out how many types of tags are there and the number of each tag.\n",
    "'''\n",
    "\n",
    "def count_tags(filename):\n",
    "    tags = {}\n",
    "    for event, element in ET.iterparse(filename):\n",
    "        if element.tag not in tags.keys():\n",
    "            tags[element.tag] = 1\n",
    "        else:\n",
    "            tags[element.tag] += 1\n",
    "    return tags\n",
    "\n",
    "def test():\n",
    "\n",
    "    tags = count_tags('san-jose_california.osm')\n",
    "    pprint.pprint(tags)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, I check for any errors in the tags by classifying them into tags with:\n",
    "\n",
    "- only lowercase\n",
    "- lowercase and colon\n",
    "- problematic characters\n",
    "- others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lower': 12, 'lower_colon': 12, 'other': 5, 'problemchars': 0}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "The code below allows you to check the k value for each tag.\n",
    "By classifying the tagss into few categories:\n",
    "1. \"lower\": valid tags containing only lowercase letters\n",
    "2. \"lower_colon\": valid tags with a colon in the names\n",
    "3. \"problemchars\": tags with problematic characters\n",
    "4. \"other\": other tags that don't fall into the 3 categories above\n",
    "'''\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "def key_type(element, keys):\n",
    "    if element.tag == \"tag\":\n",
    "        k = element.attrib['k']\n",
    "        if re.search(lower,k):\n",
    "            keys[\"lower\"] += 1\n",
    "        elif re.search(lower_colon,k):\n",
    "            keys[\"lower_colon\"] += 1\n",
    "        elif re.search(problemchars,k):\n",
    "            keys[\"problemchars\"] += 1\n",
    "        else:\n",
    "            keys[\"other\"] += 1\n",
    "    return keys\n",
    "\n",
    "def process_map(filename):\n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        keys = key_type(element, keys)\n",
    "    return keys\n",
    "\n",
    "def test():\n",
    "    keys = process_map('san-jose_california.osm')\n",
    "    pprint.pprint(keys)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auditng Street Names\n",
    "\n",
    "I construct of a list of common expected street type such as \"Street\", \"Avenue\" etc.\n",
    "\n",
    "Then, I list out all the street type not in the expected street type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x81 in position 2523: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-45fd93634fa6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mstreet_types\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[0mst_types\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maudit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mOSMFILE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-45fd93634fa6>\u001b[0m in \u001b[0;36maudit\u001b[1;34m(osmfile)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mosm_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mosmfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mstreet_types\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mevent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mET\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mosm_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"start\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"node\"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"way\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Ags91\\Anaconda3\\lib\\xml\\etree\\ElementTree.py\u001b[0m in \u001b[0;36miterator\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1222\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpullparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_events\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m                 \u001b[1;31m# load event buffer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1224\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m1024\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1225\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Ags91\\Anaconda3\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x81 in position 2523: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "'''\n",
    "The code below lists out all the street types not in the expected list.\n",
    "'''\n",
    "OSMFILE = \"san-jose_california.osm\"\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\", \"Circle\", \"Terrace\", \"Way\"]\n",
    "\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return street_types\n",
    "\n",
    "st_types = audit(OSMFILE)\n",
    "\n",
    "def test():    \n",
    "    pprint.pprint(dict(st_types))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It can be seen from the output above that there are a lot of inconsistencies and error in street type.\n",
    "\n",
    "Few examples are:\n",
    "- \"St\" (overabbreviated)\n",
    "- \"4A\" (referring to unit number)\n",
    "- \"Alameda\" (building name)\n",
    "- \"CA\" (state name)\n",
    "\n",
    "While it will be difficult to clean most inconsistencies in street type, the overabbreviated ones are easier to identify and correct. A mapping list is written by pairing the overabbreviated name to the full street type name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The code below updates the unexpected street types listed in the mapping list\n",
    "while keeping others unchanged.\n",
    "'''\n",
    "mapping = { \"St\": \"Street\",\n",
    "            \"St.\": \"Street\",\n",
    "            \"Rd.\": \"Road\",\n",
    "            \"Ave\": \"Avenue\",\n",
    "            \"Blvd\": \"Boulevard\",\n",
    "            \"Dr\": \"Drive\",\n",
    "            \"Rd\": \"Road\"\n",
    "            }\n",
    "\n",
    "def update_name(name, mapping):\n",
    "    m = street_type_re.search(name)\n",
    "    if m.group() not in expected:\n",
    "        if m.group() in mapping.keys():\n",
    "            name = re.sub(m.group(), mapping[m.group()], name)\n",
    "    return name\n",
    "\n",
    "def test():\n",
    "    for st_type, ways in st_types.iteritems():\n",
    "        for name in ways:\n",
    "            better_name = update_name(name, mapping)\n",
    "            print name, \"=>\", better_name\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auditng Zip Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This code checks for zipcode whether they begin with '94' or '95' or something else\n",
    "'''\n",
    "OSMFILE = \"san-jose_california.osm\"\n",
    "zip_type_re = re.compile(r'\\d{5}$')\n",
    "\n",
    "def audit_ziptype(zip_types, zipcode):\n",
    "    if zipcode[0:2] != 95:\n",
    "        zip_types[zipcode[0:2]].add(zipcode)\n",
    "    elif zipcode[0:2] != 94:\n",
    "        zip_types[zipcode[0:2]].add(zipcode)\n",
    "        \n",
    "def is_zipcode(elem):\n",
    "    return (elem.attrib['k'] == \"addr:postcode\")\n",
    "\n",
    "def audit_zip(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    zip_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_zipcode(tag):\n",
    "                    audit_ziptype(zip_types,tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return zip_types\n",
    "\n",
    "zip_print = audit_zip(OSMFILE)\n",
    "\n",
    "def test():    \n",
    "    pprint.pprint(dict(zip_print))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "update the zip code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This code will update non 5-digit zipcode.\n",
    "If it is 8/9-digit, only the first 5 digits are kept.\n",
    "If it has the state name in front, only the 5 digits are kept.\n",
    "If it is something else, will not change anything as it might result in error when validating the csv file.\n",
    "'''\n",
    "def update_zipcode(zipcode):\n",
    "    \"\"\"Clean postcode to a uniform format of 5 digit; Return updated postcode\"\"\"\n",
    "    if re.findall(r'^\\d{5}$', zipcode): # 5 digits 94140\n",
    "        valid_zipcode = zipcode\n",
    "        return valid_zipcode\n",
    "    elif re.findall(r'(^\\d{5})-\\d{3}$', zipcode): # 8 digits 94150-029\n",
    "        valid_zipcode = re.findall(r'(^\\d{5})-\\d{3}$', zipcode)[0]\n",
    "        return valid_zipcode\n",
    "    elif re.findall(r'(^\\d{5})-\\d{4}$', zipcode): # 9 digits 95130-0239\n",
    "        valid_zipcode = re.findall(r'(^\\d{5})-\\d{4}$', zipcode)[0]\n",
    "        return valid_zipcode\n",
    "    elif re.findall(r'CA\\s*\\d{5}', zipcode): # with state code CA 95130\n",
    "        valid_zipcode =re.findall(r'\\d{5}', zipcode)[0]  \n",
    "        return valid_zipcode  \n",
    "    else: #return default zipcode to avoid overwriting\n",
    "        return zipcode\n",
    "    \n",
    "def test_zip():\n",
    "    for zips, ways in zip_print.iteritems():\n",
    "        for name in ways:\n",
    "            better_name = update_zipcode(name)\n",
    "            print name, \"=>\", better_name\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_zip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Database into SQL\n",
    "\n",
    "5 cvs files will be created using the schema below which will be then converted into an SQL databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import codecs\n",
    "import cerberus\n",
    "import schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load schema.py\n",
    "'''\n",
    "The schema below for the 5 csv files which will be used to construct a SQL databases\n",
    "'''\n",
    "\n",
    "schema = {\n",
    "    'node': {\n",
    "        'type': 'dict',\n",
    "        'schema': {\n",
    "            'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'lat': {'required': True, 'type': 'float', 'coerce': float},\n",
    "            'lon': {'required': True, 'type': 'float', 'coerce': float},\n",
    "            'user': {'required': True, 'type': 'string'},\n",
    "            'uid': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'version': {'required': True, 'type': 'string'},\n",
    "            'changeset': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'timestamp': {'required': True, 'type': 'string'}\n",
    "        }\n",
    "    },\n",
    "    'node_tags': {\n",
    "        'type': 'list',\n",
    "        'schema': {\n",
    "            'type': 'dict',\n",
    "            'schema': {\n",
    "                'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "                'key': {'required': True, 'type': 'string'},\n",
    "                'value': {'required': True, 'type': 'string'},\n",
    "                'type': {'required': True, 'type': 'string'}\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'way': {\n",
    "        'type': 'dict',\n",
    "        'schema': {\n",
    "            'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'user': {'required': True, 'type': 'string'},\n",
    "            'uid': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'version': {'required': True, 'type': 'string'},\n",
    "            'changeset': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'timestamp': {'required': True, 'type': 'string'}\n",
    "        }\n",
    "    },\n",
    "    'way_nodes': {\n",
    "        'type': 'list',\n",
    "        'schema': {\n",
    "            'type': 'dict',\n",
    "            'schema': {\n",
    "                'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "                'node_id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "                'position': {'required': True, 'type': 'integer', 'coerce': int}\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'way_tags': {\n",
    "        'type': 'list',\n",
    "        'schema': {\n",
    "            'type': 'dict',\n",
    "            'schema': {\n",
    "                'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "                'key': {'required': True, 'type': 'string'},\n",
    "                'value': {'required': True, 'type': 'string'},\n",
    "                'type': {'required': True, 'type': 'string'}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "5 cvs files are created with the scheme as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The code below is mostly derived from Udacity Lession 13: Case study: OpenStreetMap Data [SQL]\n",
    "https://classroom.udacity.com/nanodegrees/nd002/parts/860b269a-d0b0-4f0c-8f3d-ab08865d43bf/modules/316820862075461/lessons/5436095827/concepts/54908788190923\n",
    "'''\n",
    "OSM_PATH = \"san-jose_california.osm\"\n",
    "OSMFILE = \"san-jose_california.osm\"\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\", \"Circle\", \"Terrace\", \"Way\"]\n",
    "\n",
    "mapping = { \"St\": \"Street\",\n",
    "            \"St.\": \"Street\",\n",
    "            \"Rd.\": \"Road\",\n",
    "            \"Ave\": \"Avenue\",\n",
    "            \"Blvd\": \"Boulevard\",\n",
    "            \"Dr\": \"Drive\",\n",
    "            \"Rd\": \"Road\"\n",
    "            }\n",
    "\n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\"\n",
    "\n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')\n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "SCHEMA = schema\n",
    "\n",
    "# Make sure the fields order in the csvs matches the column order in the sql table schema\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "    \n",
    "def shape_element(element, node_attr_fields=NODE_FIELDS, way_attr_fields=WAY_FIELDS,\n",
    "                  problem_chars=PROBLEMCHARS, default_tag_type='regular'):\n",
    "    \"\"\"Clean and shape node or way XML element to Python dict\"\"\"\n",
    "    node_attribs = {}\n",
    "    way_attribs = {}\n",
    "    way_nodes = []\n",
    "    tags = []  # Handle secondary tags the same way for both node and way elements\n",
    "    p=0\n",
    "    \n",
    "    if element.tag == 'node':\n",
    "        for i in NODE_FIELDS:\n",
    "            node_attribs[i] = element.attrib[i]\n",
    "        for tag in element.iter(\"tag\"):\n",
    "            node_tags_attribs = {}\n",
    "            temp = LOWER_COLON.search(tag.attrib['k'])\n",
    "            is_p = PROBLEMCHARS.search(tag.attrib['k'])\n",
    "            if is_p:\n",
    "                continue\n",
    "            elif temp:\n",
    "                split_char = temp.group(1)\n",
    "                split_index = tag.attrib['k'].index(split_char)\n",
    "                type1 = temp.group(1)\n",
    "                node_tags_attribs['id'] = element.attrib['id']\n",
    "                node_tags_attribs['key'] = tag.attrib['k'][split_index+2:]\n",
    "                node_tags_attribs['value'] = tag.attrib['v']\n",
    "                node_tags_attribs['type'] = tag.attrib['k'][:split_index+1]\n",
    "                if node_tags_attribs['type'] == \"addr\" and node_tags_attribs['key'] == \"street\":\n",
    "                    # update street name\n",
    "                    node_tags_attribs['value'] = update_name(tag.attrib['v'], mapping) \n",
    "                #elif node_tags_attribs['type'] == \"addr\" and node_tags_attribs['key'] == \"postcode\":\n",
    "                #    # update post code\n",
    "                #    node_tags_attribs['value'] = update_zipcode(tag.attrib['v']) \n",
    "            else:\n",
    "                node_tags_attribs['id'] = element.attrib['id']\n",
    "                node_tags_attribs['key'] = tag.attrib['k']\n",
    "                node_tags_attribs['value'] = tag.attrib['v']\n",
    "                node_tags_attribs['type'] = 'regular'\n",
    "                if node_tags_attribs['type'] == \"addr\" and node_tags_attribs['key'] == \"street\":\n",
    "                    # update street name\n",
    "                    node_tags_attribs['value'] = update_name(tag.attrib['v'], mapping) \n",
    "                #elif node_tags_attribs['type'] == \"addr\" and node_tags_attribs['key'] == \"postcode\":\n",
    "                #    # update post code\n",
    "                #    node_tags_attribs['value'] = update_zipcode(tag.attrib['v']) \n",
    "            tags.append(node_tags_attribs)\n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "    elif element.tag == 'way':\n",
    "        id = element.attrib['id']\n",
    "        for i in WAY_FIELDS:\n",
    "            way_attribs[i] = element.attrib[i]\n",
    "        for i in element.iter('nd'):\n",
    "            d = {}\n",
    "            d['id'] = id\n",
    "            d['node_id'] = i.attrib['ref']\n",
    "            d['position'] = p\n",
    "            p+=1\n",
    "            way_nodes.append(d)\n",
    "        for c in element.iter('tag'):\n",
    "            temp = LOWER_COLON.search(c.attrib['k'])\n",
    "            is_p = PROBLEMCHARS.search(c.attrib['k'])\n",
    "            e = {}\n",
    "            if is_p:\n",
    "                continue\n",
    "            elif temp:\n",
    "                split_char = temp.group(1)\n",
    "                split_index = c.attrib['k'].index(split_char)\n",
    "                e['id'] = id\n",
    "                e['key'] = c.attrib['k'][split_index+2:]\n",
    "                e['type'] = c.attrib['k'][:split_index+1]\n",
    "                e['value'] = c.attrib['v']\n",
    "                if e['type'] == \"addr\" and e['key'] == \"street\":\n",
    "                    e['value'] = update_name(c.attrib['v'], mapping) \n",
    "                #elif e['type'] == \"addr\" and e['key'] == \"postcode\":\n",
    "                #    e['value'] = update_zipcode(c.attrib['v'])\n",
    "            else:\n",
    "                e['id'] = id\n",
    "                e['key'] = c.attrib['k']\n",
    "                e['type'] = 'regular'\n",
    "                e['value'] =  c.attrib['v']\n",
    "                if e['type'] == \"addr\" and e['key'] == \"street\":\n",
    "                    e['value'] = update_name(c.attrib['v'], mapping) \n",
    "                #elif e['type'] == \"addr\" and e['key'] == \"postcode\":\n",
    "                #    e['value'] = update_zipcode(c.attrib['v'])\n",
    "            tags.append(e)\n",
    "        \n",
    "    return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n",
    "        \n",
    "    if element.tag == 'node':\n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "    elif element.tag == 'way':\n",
    "        return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n",
    "    \n",
    "\n",
    "# ================================================== #\n",
    "#               Helper Functions                     #\n",
    "# ================================================== #\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\"\"\"\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "def validate_element(element, validator, schema=SCHEMA):\n",
    "    \"\"\"Raise ValidationError if element does not match schema\"\"\"\n",
    "    if validator.validate(element, schema) is not True:\n",
    "        field, errors = next(validator.errors.iteritems())\n",
    "        message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\"\n",
    "        error_string = pprint.pformat(errors)\n",
    "        \n",
    "        raise Exception(message_string.format(field, error_string))\n",
    "\n",
    "\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    \"\"\"Extend csv.DictWriter to handle Unicode input\"\"\"\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: (v.encode('utf-8') if isinstance(v, unicode) else v) for k, v in row.iteritems()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Main Function                        #\n",
    "# ================================================== #\n",
    "def process_map(file_in, validate):\n",
    "    \"\"\"Iteratively process each XML element and write to csv(s)\"\"\"\n",
    "    with codecs.open(NODES_PATH, 'wb') as nodes_file, \\\n",
    "        codecs.open(NODE_TAGS_PATH, 'wb') as nodes_tags_file, \\\n",
    "        codecs.open(WAYS_PATH, 'wb') as ways_file, \\\n",
    "        codecs.open(WAY_NODES_PATH, 'wb') as way_nodes_file, \\\n",
    "        codecs.open(WAY_TAGS_PATH, 'wb') as way_tags_file:\n",
    "\n",
    "        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "\n",
    "        validator = cerberus.Validator()\n",
    "\n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                if validate is True:\n",
    "                    validate_element(el, validator)\n",
    "\n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    node_tags_writer.writerows(el['node_tags'])\n",
    "                elif element.tag == 'way':\n",
    "                    ways_writer.writerow(el['way'])\n",
    "                    way_nodes_writer.writerows(el['way_nodes'])\n",
    "                    way_tags_writer.writerows(el['way_tags'])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Note: Validation is ~ 10X slower. For the project consider using a small\n",
    "    # sample of the map when validating.\n",
    "    process_map(OSM_PATH, validate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The 5 csv files are then converted into a .db database using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the database and tables\n",
    "import sqlite3\n",
    "conn = sqlite3.connect('data_wrangling.sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.text_factory = str\n",
    "cur = conn.cursor()\n",
    "\n",
    "#Make some fresh tables using executescript()\n",
    "cur.execute('''DROP TABLE IF EXISTS nodes''')\n",
    "cur.execute('''DROP TABLE IF EXISTS nodes_tags''')\n",
    "cur.execute('''DROP TABLE IF EXISTS ways''')\n",
    "cur.execute('''DROP TABLE IF EXISTS ways_tags''')\n",
    "cur.execute('''DROP TABLE IF EXISTS ways_nodes''')\n",
    "\n",
    "\n",
    "cur.execute('''CREATE TABLE nodes (\n",
    "    id INTEGER PRIMARY KEY NOT NULL,\n",
    "    lat REAL,\n",
    "    lon REAL,\n",
    "    user TEXT,\n",
    "    uid INTEGER,\n",
    "    version INTEGER,\n",
    "    changeset INTEGER,\n",
    "    timestamp TEXT)\n",
    "''')\n",
    "\n",
    "with open('nodes.csv','r') as nodes_table: # `with` statement available in 2.5+\n",
    "    # csv.DictReader uses first line in file for column headings by default\n",
    "    dr = csv.DictReader(nodes_table) # comma is default delimiter\n",
    "    to_db = [(i['id'], i['lat'], i['lon'], i['user'], i['uid'], i['version'], i['changeset'], i['timestamp']) for i in dr]\n",
    "\n",
    "cur.executemany(\"INSERT INTO nodes VALUES (?, ?, ?, ?, ?, ?, ?, ?);\", to_db)\n",
    "\n",
    "\n",
    "cur.execute('''CREATE TABLE nodes_tags (\n",
    "    id INTEGER,\n",
    "    key TEXT,\n",
    "    value TEXT,\n",
    "    type TEXT,\n",
    "    FOREIGN KEY (id) REFERENCES nodes(id))\n",
    "''')\n",
    "\n",
    "with open('nodes_tags.csv','r') as nodes_tags_table: # `with` statement available in 2.5+\n",
    "    # csv.DictReader uses first line in file for column headings by default\n",
    "    dr = csv.DictReader(nodes_tags_table) # comma is default delimiter\n",
    "    to_db = [(i['id'], i['key'], i['value'], i['type']) for i in dr]\n",
    "\n",
    "cur.executemany(\"INSERT INTO nodes_tags VALUES (?, ?, ?, ?);\", to_db)\n",
    "\n",
    "cur.execute('''CREATE TABLE ways (\n",
    "    id INTEGER PRIMARY KEY NOT NULL,\n",
    "    user TEXT,\n",
    "    uid INTEGER,\n",
    "    version TEXT,\n",
    "    changeset INTEGER,\n",
    "    timestamp TEXT)\n",
    "''')\n",
    "\n",
    "with open('ways.csv','r') as ways_table: # `with` statement available in 2.5+\n",
    "    # csv.DictReader uses first line in file for column headings by default\n",
    "    dr = csv.DictReader(ways_table) # comma is default delimiter\n",
    "    to_db = [(i['id'], i['user'], i['uid'], i['version'], i['changeset'], i['timestamp']) for i in dr]\n",
    "\n",
    "cur.executemany(\"INSERT INTO ways VALUES (?, ?, ?, ?, ?, ?);\", to_db)\n",
    "\n",
    "cur.execute('''CREATE TABLE ways_tags (\n",
    "    id INTEGER NOT NULL,\n",
    "    key TEXT NOT NULL,\n",
    "    value TEXT NOT NULL,\n",
    "    type TEXT,\n",
    "    FOREIGN KEY (id) REFERENCES ways(id))\n",
    "''')\n",
    "\n",
    "with open('ways_tags.csv','r') as ways_tags_table: # `with` statement available in 2.5+\n",
    "    # csv.DictReader uses first line in file for column headings by default\n",
    "    dr = csv.DictReader(ways_tags_table) # comma is default delimiter\n",
    "    to_db = [(i['id'], i['key'], i['value'], i['type']) for i in dr]\n",
    "\n",
    "cur.executemany(\"INSERT INTO ways_tags VALUES (?, ?, ?, ?);\", to_db)\n",
    "\n",
    "cur.execute('''CREATE TABLE ways_nodes (\n",
    "    id INTEGER NOT NULL,\n",
    "    node_id INTEGER NOT NULL,\n",
    "    position INTEGER NOT NULL,\n",
    "    FOREIGN KEY (id) REFERENCES ways(id),\n",
    "    FOREIGN KEY (node_id) REFERENCES nodes(id))\n",
    "''')\n",
    "\n",
    "with open('ways_nodes.csv','r') as ways_nodes_table: # `with` statement available in 2.5+\n",
    "    # csv.DictReader uses first line in file for column headings by default\n",
    "    dr = csv.DictReader(ways_nodes_table) # comma is default delimiter\n",
    "    to_db = [(i['id'], i['node_id'], i['position']) for i in dr]\n",
    "\n",
    "cur.executemany(\"INSERT INTO ways_nodes VALUES (?, ?, ?);\", to_db)\n",
    "\n",
    "#Save changes\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Overviews\n",
    "\n",
    "The queries below provides an overview of the San Jose, CA OpenStreetMap dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print ('The san-jose_california.osm file is {} MB'.format(os.path.getsize('san-jose_california.osm')/1.0e6))\n",
    "print ('The nodes.csv file is {} MB'.format(os.path.getsize('nodes.csv')/1.0e6))\n",
    "print ('The nodes_tags.csv file is {} MB'.format(os.path.getsize('nodes_tags.csv')/1.0e6))\n",
    "print ('The ways.csv file is {} MB'.format(os.path.getsize('ways.csv')/1.0e6))\n",
    "print ('The ways_nodes.csv file is {} MB'.format(os.path.getsize('ways_nodes.csv')/1.0e6))\n",
    "print ('The ways_tags.csv file is {} MB'.format(os.path.getsize('ways_tags.csv')/1.0e6))\n",
    "print ('The data_wrangling.sqlite file is {} MB'.format(os.path.getsize('data_wrangling.sqlite')/1.0e6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_count = conn.cursor()\n",
    "nodes_count.execute(\"SELECT COUNT(*) FROM nodes\")\n",
    "\n",
    "nodes_count = nodes_count.fetchall()\n",
    " \n",
    "for row in nodes_count:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ways_count = conn.cursor()\n",
    "ways_count.execute(\"SELECT COUNT(*) FROM ways\")\n",
    "\n",
    "ways_count = ways_count.fetchall()\n",
    " \n",
    "for row in ways_count:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of unique users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_user = conn.cursor()\n",
    "unique_user.execute(\"SELECT COUNT(DISTINCT(e.uid)) FROM (SELECT uid FROM nodes UNION ALL SELECT uid FROM ways) e\")\n",
    "\n",
    "unique_user = unique_user.fetchall()\n",
    " \n",
    "for row in unique_user:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 5 user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_user = conn.cursor()\n",
    "top_user.execute(\"SELECT e.user, COUNT(*) as num FROM (SELECT user FROM nodes UNION ALL SELECT user FROM ways) e GROUP BY e.user ORDER BY num DESC LIMIT 5\")\n",
    "\n",
    "top_user = top_user.fetchall()\n",
    " \n",
    "for row in top_user:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 Amenities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_amenities = conn.cursor()\n",
    "t = ('amenity',)\n",
    "top_amenities.execute(\"SELECT value, COUNT(*) as num FROM nodes_tags WHERE key=? GROUP BY value ORDER BY num DESC LIMIT 10\", t)\n",
    "\n",
    "top_amenities = top_amenities.fetchall()\n",
    " \n",
    "for row in top_amenities:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Popular Sports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_sports = conn.cursor()\n",
    "t = ('sport',)\n",
    "top_sports.execute(\"SELECT value, COUNT(*) as num FROM nodes_tags WHERE key=? GROUP BY value ORDER BY num DESC LIMIT 10\", t)\n",
    "\n",
    "top_sports = top_sports.fetchall()\n",
    " \n",
    "for row in top_sports:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Tourism-related Amenties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_tourism = conn.cursor()\n",
    "t = ('tourism', 'tourism_1')\n",
    "top_tourism.execute(\"SELECT value, COUNT(*) as num FROM nodes_tags WHERE key=? GROUP BY value UNION SELECT value, COUNT(*) as num FROM nodes_tags WHERE key=? GROUP BY value ORDER BY num DESC\", t)\n",
    "\n",
    "top_tourism = top_tourism.fetchall()\n",
    " \n",
    "for row in top_tourism:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top City Apprearance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_city = conn.cursor()\n",
    "t = ('city',)\n",
    "top_city.execute(\"SELECT value, COUNT(*) as num FROM nodes_tags WHERE key=? GROUP BY value ORDER BY num DESC LIMIT 10\", t)\n",
    "\n",
    "top_city = top_city.fetchall()\n",
    " \n",
    "for row in top_city:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Payment Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_payment = conn.cursor()\n",
    "t = ('payment',)\n",
    "top_payment.execute(\"SELECT key, COUNT(*) as num FROM nodes_tags WHERE type=? GROUP BY key ORDER BY num DESC\", t)\n",
    "\n",
    "top_payment = top_payment.fetchall()\n",
    " \n",
    "for row in top_payment:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the 'nodes_tag' file, there seems to be a lot of inconsistencies or incorrect value.\n",
    "\n",
    "As an open-sourced project, there isn't much restriction placed on data editing. As such, the integrity of the data is easily compromised when there are a lot of users.\n",
    "\n",
    "Few recommended solutions that can be implemented are:\n",
    "- Enforce compulsory columns where users have to fill in all columns in order to submit an entry\n",
    "- Cross-referencing missing data from other open-sourced data from governmental agencies or API\n",
    "- Restructure the schema of the databases so that users are clear what to enter (most useful for address where zipcode or state should have separate columns)\n",
    "- Implement some sort of artificial intelligence to determine potential errors in the entries (such as overabbreviated street types)\n",
    "- Have a bot to update/create entries using data from other sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benefits\n",
    "\n",
    "+ By having a more complete dataset, it can serve as a place to search for places or info for each city\n",
    "+ With proper transportation related ways_tags data, public transportation data can be included\n",
    "+ Cost of manually cleaning or editing errorneous data is high, and the use of a bot or machine learning might help to speed up and bring down the cost in the long run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anticipated Problems\n",
    "\n",
    "- As an open-sourced platform, cost and human reousrces are main issues\n",
    "- Lack of awareness of the existence OSM means it has to be self-sufficient and sustainable in its project taking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "The OpenStreetMap data is a great place for users to contribute to the data. However, if the integrity and completenenss of data are not maintained, it can't compete with other map services such as Google Maps or Apple Maps which has a better user experience and more structure ways of searching for info.\n",
    "\n",
    "With more care taken, OpenStreetMap is capable of being the Wikipedia of maps."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
